{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb70a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11. 22.]\n",
      " [13. 24.]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data=None, requires_grad=False):\n",
    "        # self.data = self._data_to_numpy(data)\n",
    "        # Float32 to keep consistency. \n",
    "        self.data = np.array(data, dtype=np.float32) #Since numpy already handles type conversion, let's use it to keep code simple.\n",
    "        self.shape = self.data.shape\n",
    "        self.size = self.data.size\n",
    "        self.dtype = self.data.dtype\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "  \n",
    "    def _data_to_numpy(self, data):\n",
    "        import numpy as np\n",
    "        if isinstance(data, list):\n",
    "            return np.array(data)\n",
    "    def to_numpy(self):\n",
    "        return self.data\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, shape={self.shape}, size={self.size}, dtype={self.dtype}, requires_grad={self.requires_grad})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Custom addition for Tensor objects to create a new Tensor object\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data + other.data)\n",
    "            # return self.data + other.data\n",
    "        elif isinstance(other, (int, float)): #Broadcasting support\n",
    "            return Tensor(self.data + other)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\")\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Custom subtraction for Tensor objects to create a new Tensor object\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data - other.data)\n",
    "        elif isinstance(other, (int, float)): #Broadcasting support\n",
    "            return Tensor(self.data - other)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operand type(s) for -: '{type(self)}' and '{type(other)}'\")\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Custom multiplication for Tensor objects to create a new Tensor object\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data)\n",
    "        elif isinstance(other, (int, float)): #Broadcasting support\n",
    "            return Tensor(self.data * other)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operand type(s) for *: '{type(self)}' and '{type(other)}'\")\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data / other.data)\n",
    "        elif isinstance(other, (int, float)): #Broadcasting support\n",
    "            if other == 0: #Can be floating point zero?\n",
    "                raise ZeroDivisionError(\"Cannot divide by zero\")\n",
    "            return Tensor(self.data / other)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operand type(s) for /: '{type(self)}' and '{type(other)}'\")\n",
    "    # def matmul(self, other):\n",
    "    #     \"\"\"For dealing with matrix, numpy uses row first operations i.e., \n",
    "    #     if a = [[1,2,3], [4,5,6]] -> a.flat will return flat array in order [1,2,3,4,5,6] instead of [1,4,2,5,3,6]\"\"\"\n",
    "    #     if isinstance(other, Tensor):\n",
    "    #         # Shape will be dynamic, can be one, two, three and so on...\n",
    "    #         # Note: matrix multiplication in mathematics is only allowed by d dimemnsions. But tensor = multiple dimensions. Check defination of matrix.\n",
    "    #         # So, broadcast rules is applied to tensors, to define what can be multiplies similar to a matrix. \n",
    "    #         # The order of matrix multiplication matters\n",
    "            \n",
    "    #         # Track if we reshaped inputs from 1D\n",
    "    #         other_was_1d = len(other.data.shape) == 1\n",
    "    #         self_was_1d = len(self.data.shape) == 1\n",
    "            \n",
    "    #         # Work with local copies to avoid modifying the original tensors\n",
    "    #         self_data = self.data\n",
    "    #         other_data = other.data\n",
    "            \n",
    "    #         if self_was_1d:\n",
    "    #             self_data = self.data.reshape(1, -1)\n",
    "    #             print(f\"The new shape of self is {self_data.shape}\")\n",
    "    #         if other_was_1d:\n",
    "    #             other_data = other.data.reshape(-1, 1)\n",
    "    #             print(f\"The new shape of other is {other_data.shape}\")\n",
    "\n",
    "    #         if self_data.shape[1] != other_data.shape[0]:\n",
    "    #             # raise ValueError(\"Matrix dimensions do not match\")\n",
    "    #             raise ValueError(f\"Inner dimensions must match for matrix multiplication: {self_data.shape[1]} â‰  {other_data.shape[0]}\")\n",
    "    #         new_matrix = np.zeros((self_data.shape[0], other_data.shape[1]))\n",
    "    #         print(f\"Shape of new matrix is: {new_matrix.shape}\")\n",
    "\n",
    "    #         # for val in self.data.flat:\n",
    "    #         for first_mat_row in range(self_data.shape[0]):\n",
    "    #             for second_mat_column in range(other_data.shape[1]):\n",
    "    #                 for common_internal in range(self_data.shape[1]): #or other_data.shape[0]\n",
    "    #                     new_matrix[first_mat_row][second_mat_column] += self_data[first_mat_row][common_internal]*other_data[common_internal][second_mat_column]\n",
    "            \n",
    "    #         # If other was originally 1D, result should be 1D (squeeze the column dimension)\n",
    "    #         if other_was_1d and new_matrix.shape[1] == 1:\n",
    "    #             new_matrix = new_matrix.squeeze(axis=1)  # Convert from (n, 1) to (n,)\n",
    "            \n",
    "    #         return Tensor(new_matrix)\n",
    "\n",
    "    \n",
    "    def reshape(self, *args):\n",
    "        #Flatten the input into flat numbers\n",
    "        def flatten_args(args):\n",
    "            for arg in args:\n",
    "                yield arg\n",
    "        args_list = list(flatten_args(args)) \n",
    "        if np.prod(np.array(args_list)) != self.size:\n",
    "            raise ValueError(f\"The new shape {args_list} does not match the old shape {self.data.shape}\")\n",
    "        return Tensor(np.reshape(self.data, tuple(args_list)))\n",
    "        # Note: We can reshape by creating a copy, but to do it wihout creating copy, then we need to interpret how the data is stored in memory: C, F foramt and the stride.\n",
    "        # First property: the multiplication of the new shape should be always equals to old shape\n",
    "    \n",
    "    # Note: The case fails when the input is a vector of shape(3,) so we can convert to (3,1) and then after algorithm, convert back to (3,)\n",
    "    # Note: The case do not handles higher dimensions than 2d matrix: Have to recursively perofrm matrix multiplication.\n",
    "    # If we have used np.matmul(x,y ) it automatically have handled all of that!!\n",
    "    def matmul(self, other):\n",
    "        \"\"\"For dealing with matrix, numpy uses row first operations i.e., \n",
    "        if a = [[1,2,3], [4,5,6]] -> a.flat will return flat array in order [1,2,3,4,5,6] instead of [1,4,2,5,3,6]\"\"\"\n",
    "        if isinstance(other, Tensor):\n",
    "            # Shape will be dynamic, can be one, two, three and so on...\n",
    "            # Note: matrix multiplication in mathematics is only allowed by d dimemnsions. But tensor = multiple dimensions. Check defination of matrix.\n",
    "            # So, broadcast rules is applied to tensors, to define what can be multiplies similar to a matrix. \n",
    "            # The order of matrix multiplication matters\n",
    "            if len(self.data.shape) == 1:\n",
    "                self.shape = self.data.reshape(1, -1).shape\n",
    "                print(f\"The new shape of self is {self.shape}\")\n",
    "            if len(other.data.shape) == 1:\n",
    "                other.shape = other.data.reshape(-1, 1).shape\n",
    "                print(f\"The new shape of other is {other.shape}\")\n",
    "\n",
    "            if self.shape[1] != other.shape[0]:\n",
    "                raise ValueError(\"Matrix dimensions do not match\")\n",
    "            new_matrix = np.zeros((self.shape[0], other.shape[1]))\n",
    "            print(f\"Shape of new matrix is: {new_matrix.shape}\")\n",
    "\n",
    "            # for val in self.data.flat:\n",
    "            for first_mat_row in range(self.shape[0]):\n",
    "                for second_mat_column in range(other.shape[1]):\n",
    "                    for common_internal in range(self.shape[1]): #or other.shape[0]\n",
    "                        new_matrix[first_mat_row][second_mat_column] += self.data[first_mat_row][common_internal]*other.data[common_internal][second_mat_column]\n",
    "            return Tensor(new_matrix)\n",
    "            # return Tensor(np.matmul(self.data, other.data))\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data * other)\n",
    "    # Note: So numpy, pytorch uses BLAS, cuBLAS library to handle matrix multiplication. It is 10000x faster than The current normal approach \n",
    "    # It still takes O(n^3) time but with caching, hardware optimised reading (and SIMD utilisation with multiple cores or threads), vectorisation\n",
    "\n",
    "    \n",
    "h = Tensor([1,2,3])\n",
    "z = Tensor([4,5,6])\n",
    "matrix = Tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
    "vector = Tensor([10, 20])          # Shape: (2,)\n",
    "result = matrix + vector           # Broadcasting: (2,2) + (2,) â†’ (2,2)\n",
    "print(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18c869b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for val in range(1):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea4c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5cc5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only specify one unknown dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: can only specify one unknown dimension"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vec = np.array([1,2,3])\n",
    "# vec.reshape((1,1,-1,1,3,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98dc767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (2, 3)\n"
     ]
    }
   ],
   "source": [
    "def reshape2(*args, **kwargs):\n",
    "    # args = tuple(args)\n",
    "    print(type(args), args)\n",
    "reshape2(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f7c56bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape of other is (3, 1)\n",
      "Shape of new matrix is: (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[14.]\n",
       " [32.]], shape=(2, 1), size=2, dtype=float32, requires_grad=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor([[1,2,3],[4,5,6]]).data[1][1]\n",
    "# a = Tensor([[1,1,2],[1,2,1]])\n",
    "# b = Tensor([[1,1],[1,2],[2,2]])\n",
    "matrix = Tensor([[1, 2, 3], [4, 5, 6]])  # 2Ã—3\n",
    "vector = Tensor([1, 2, 3])  # 3Ã—1 (conceptually)\n",
    "result = matrix.matmul(vector)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf8da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2a905cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(vector.data.reshape(1,-1).shape)\n",
    "# vector2 = Tensor([1,2,3])\n",
    "import numpy as np\n",
    "vector2 = np.array([1,2,3])\n",
    "vector2.reshape(((((1, -1))))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "585e93d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5272369384765625e-05\n",
      "2.4080276489257812e-05\n",
      "False 0.9528301886792453\n"
     ]
    }
   ],
   "source": [
    "def flatten_recursive(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            yield from flatten_recursive(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def flatten_stack(seq):\n",
    "    stack = [iter(seq)]\n",
    "    while stack:\n",
    "        for item in stack[-1]:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                stack.append(iter(item))\n",
    "                break\n",
    "            else:\n",
    "                yield item\n",
    "        else:\n",
    "            stack.pop()\n",
    "import time\n",
    "start= time.time()\n",
    "list(flatten_recursive(((((((((((((((((((((((((1,2,3))))))))))))))))))))))))))  \n",
    "p = time.time() - start\n",
    "print(p)\n",
    "start = time.time()\n",
    "list(flatten_stack(((((((((((((((((((((((((1,2,3))))))))))))))))))))))))))\n",
    "h = time.time()-start\n",
    "print(h)\n",
    "print(h>p, h/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def flatten_recursive(seq):\n",
    "    for item in seq:\n",
    "        if item ==1:\n",
    "            yield from flatten_recursive(item)\n",
    "        # if isinstance(item, (list, tuple)):\n",
    "        #     yield from flatten_recursive(item)\n",
    "        else:\n",
    "            yield item\n",
    "flatten_recursive((((((((((((((((((((((((((1,2,3))))))))))))))))))))))))))\n",
    "# for item in (1,1,1,1,1,2,2,3,1,1,1,1,1):\n",
    "#     print(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743db683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_recursive(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            yield from flatten_recursive(item)\n",
    "        else:\n",
    "            yield item\n",
    "nested = [1, [2, [3, 4], 5], (6, 7, [8, 9, [10]])]\n",
    "print(list(flatten_recursive(nested)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f301bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = Tensor([[1,2,3]])\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a361a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Tensor Creation...\n",
      "âœ… Tensor creation works correctly!\n",
      "ðŸ§ª Unit Test: Arithmetic Operations...\n",
      "âœ… Arithmetic operations work correctly!\n",
      "ðŸ§ª Unit Test: Matrix Multiplication...\n",
      "Shape of new matrix is: (2, 2)\n",
      "Shape of new matrix is: (2, 2)\n",
      "The new shape of other is (3, 1)\n",
      "Shape of new matrix is: (2, 1)\n",
      "âœ… Matrix multiplication works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_tensor_creation():\n",
    "    \"\"\"ðŸ§ª Test Tensor creation with various data types.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Tensor Creation...\")\n",
    "\n",
    "    # Test scalar creation\n",
    "    scalar = Tensor(5.0)\n",
    "    assert scalar.data == 5.0\n",
    "    assert scalar.shape == ()\n",
    "    assert scalar.size == 1\n",
    "    assert scalar.requires_grad == False\n",
    "    assert scalar.grad is None\n",
    "    assert scalar.dtype == np.float32\n",
    "\n",
    "    # Test vector creation\n",
    "    vector = Tensor([1, 2, 3])\n",
    "    assert np.array_equal(vector.data, np.array([1, 2, 3], dtype=np.float32))\n",
    "    assert vector.shape == (3,)\n",
    "    assert vector.size == 3\n",
    "\n",
    "    # Test matrix creation\n",
    "    matrix = Tensor([[1, 2], [3, 4]])\n",
    "    assert np.array_equal(matrix.data, np.array([[1, 2], [3, 4]], dtype=np.float32))\n",
    "    assert matrix.shape == (2, 2)\n",
    "    assert matrix.size == 4\n",
    "\n",
    "    # Test gradient flag (dormant feature)\n",
    "    grad_tensor = Tensor([1, 2], requires_grad=True)\n",
    "    assert grad_tensor.requires_grad == True\n",
    "    assert grad_tensor.grad is None  # Still None until Module 05\n",
    "\n",
    "    print(\"âœ… Tensor creation works correctly!\")\n",
    "\n",
    "\n",
    "test_unit_tensor_creation()\n",
    "\n",
    "def test_unit_arithmetic_operations():\n",
    "    \"\"\"ðŸ§ª Test arithmetic operations with broadcasting.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Arithmetic Operations...\")\n",
    "\n",
    "    # Test tensor + tensor\n",
    "    a = Tensor([1, 2, 3])\n",
    "    b = Tensor([4, 5, 6])\n",
    "    result = a + b\n",
    "    assert np.array_equal(result.data, np.array([5, 7, 9], dtype=np.float32))\n",
    "\n",
    "    # Test tensor + scalar (very common in ML)\n",
    "    result = a + 10\n",
    "    assert np.array_equal(result.data, np.array([11, 12, 13], dtype=np.float32))\n",
    "\n",
    "    # Test broadcasting with different shapes (matrix + vector)\n",
    "    matrix = Tensor([[1, 2], [3, 4]])\n",
    "    vector = Tensor([10, 20])\n",
    "    result = matrix + vector\n",
    "    expected = np.array([[11, 22], [13, 24]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test subtraction (data centering)\n",
    "    result = b - a\n",
    "    assert np.array_equal(result.data, np.array([3, 3, 3], dtype=np.float32))\n",
    "\n",
    "    # Test multiplication (scaling)\n",
    "    result = a * 2\n",
    "    assert np.array_equal(result.data, np.array([2, 4, 6], dtype=np.float32))\n",
    "\n",
    "    # Test division (normalization)\n",
    "    result = b / 2\n",
    "    assert np.array_equal(result.data, np.array([2.0, 2.5, 3.0], dtype=np.float32))\n",
    "\n",
    "    # Test chaining operations (common in ML pipelines)\n",
    "    normalized = (a - 2) / 2  # Center and scale\n",
    "    expected = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n",
    "    assert np.allclose(normalized.data, expected)\n",
    "\n",
    "    print(\"âœ… Arithmetic operations work correctly!\")\n",
    "\n",
    "\n",
    "test_unit_arithmetic_operations()\n",
    "\n",
    "def test_unit_matrix_multiplication():\n",
    "    \"\"\"ðŸ§ª Test matrix multiplication operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Matrix Multiplication...\")\n",
    "\n",
    "    # Test 2Ã—2 matrix multiplication (basic case)\n",
    "    a = Tensor([[1, 2], [3, 4]])  # 2Ã—2\n",
    "    b = Tensor([[5, 6], [7, 8]])  # 2Ã—2\n",
    "    result = a.matmul(b)\n",
    "    # Expected: [[1Ã—5+2Ã—7, 1Ã—6+2Ã—8], [3Ã—5+4Ã—7, 3Ã—6+4Ã—8]] = [[19, 22], [43, 50]]\n",
    "    expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test rectangular matrices (common in neural networks)\n",
    "    c = Tensor([[1, 2, 3], [4, 5, 6]])  # 2Ã—3 (like batch_size=2, features=3)\n",
    "    d = Tensor([[7, 8], [9, 10], [11, 12]])  # 3Ã—2 (like features=3, outputs=2)\n",
    "    result = c.matmul(d)\n",
    "    # Expected: [[1Ã—7+2Ã—9+3Ã—11, 1Ã—8+2Ã—10+3Ã—12], [4Ã—7+5Ã—9+6Ã—11, 4Ã—8+5Ã—10+6Ã—12]]\n",
    "    expected = np.array([[58, 64], [139, 154]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test matrix-vector multiplication (common in forward pass)\n",
    "    matrix = Tensor([[1, 2, 3], [4, 5, 6]])  # 2Ã—3\n",
    "    vector = Tensor([1, 2, 3])  # 3Ã—1 (conceptually)\n",
    "    result = matrix.matmul(vector)\n",
    "    # Expected: [1Ã—1+2Ã—2+3Ã—3, 4Ã—1+5Ã—2+6Ã—3] = [14, 32]\n",
    "    expected = np.array([14, 32], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test shape validation - should raise clear error\n",
    "    try:\n",
    "        incompatible_a = Tensor([[1, 2]])     # 1Ã—2\n",
    "        incompatible_b = Tensor([[1], [2], [3]])  # 3Ã—1\n",
    "        incompatible_a.matmul(incompatible_b)  # 1Ã—2 @ 3Ã—1 should fail (2 â‰  3)\n",
    "        assert False, \"Should have raised ValueError for incompatible shapes\"\n",
    "    except ValueError as e:\n",
    "        assert \"Inner dimensions must match\" in str(e)\n",
    "        assert \"2 â‰  3\" in str(e)  # Should show specific dimensions\n",
    "\n",
    "    print(\"âœ… Matrix multiplication works correctly!\")\n",
    "\n",
    "\n",
    "test_unit_matrix_multiplication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df66799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_fb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
